{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __cpp_optim__\n",
    "This repository contains basic implementations of numerical optimization algorithms for nonlinear problems. As testbed two simple problems are provided.\n",
    "\n",
    "##  __Getting started__\n",
    "You need a C++ compiler (like GCC on Linux) as well as the [Eigen3](http://eigen.tuxfamily.org/index.php?title=Main_Page) template library. The code in this repo uses Eigen 3.3.7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Implemented Optimization Algorithms__\n",
    "This repo implements three numerical optimization algorithms for nonlinear functions. For each algorithm the step size is determined by backtracking line search using the [Armijo-rule](https://scicomp.stackexchange.com/questions/5478/confusion-about-armijo-rule). \n",
    "\n",
    "1. __Plain Gradient Descent__  \n",
    "    This is a plain implementation of the _steepest descent method_. Further information can be read [here](https://en.wikipedia.org/wiki/Gradient_descent). It uses the negative gradient of the function $f$ at a point $x$ as search direction: $d^k = - \\nabla f(x^k)$.\n",
    "\n",
    "2. __Gauss-Newton Method__  \n",
    "    The _Gauss-Newton algorithm_ is a method for finding the minimum of nonlinear least squares problems of the shape $f(x) = \\frac{1}{2}||r(x)||^2_2$ where $r(x)$ is a linear function. The objective is thus convex. The search direction in the _Gauss-Newton-Algorithm_ is determined by $d^k = - \\nabla_{A^k} f(x^k)$ with $A^k = \\nabla r (x^k) Dr(x^k)$. $Dr(x^k)$ here denotes the Jacobian and $\\nabla r (x^k)$ the transposed Jacobian. For more information, read [here](https://en.wikipedia.org/wiki/Gauss%E2%80%93Newton_algorithm).\n",
    "\n",
    "3. __Conjugate Gradient Method__  \n",
    "    This is an implementation of the [nonlinear conjugate gradient method](https://en.wikipedia.org/wiki/Nonlinear_conjugate_gradient_method). The search direction here is given as \n",
    "    $d^{k+1} = - \\nabla f(x^{k+1}) + \\alpha \\cdot d^k$. The parameter $\\alpha$ is calculated according to the formula of Fletcher-Reeves: $\\alpha = \\frac{ ||\\nabla f(x^{k+1})||^2_2 }{ ||\\nabla f(x^k)||^2_2 }$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Example problems__\n",
    "Two toy problems are implemented to test the algorithms.  \n",
    "\n",
    "1. __Location of a signal__  \n",
    "    Given are the Locations of 5 sensors $s^m$. Each Sensor measures the distance to a signal source (e.g. a mobile phone) $\\hat{x}$. However, the measurements are noisy with error terms $u_i$. A solution to this problem can be found by finding a critical point of the following (non-convex) function:\n",
    "    $$\n",
    "    SL: \\quad \\min_{x\\in \\mathbb{R}^n} h(x) \\doteq \\sum_{i=1}^m \\big(||x-s^i||_2^2-u^2_i \\big)^2\n",
    "    $$\n",
    "    The problem can be solved using the Conjugate Gradient Method or Plain Gradient Descent.  \n",
    "\n",
    "2. __Nonlinear least squares__  \n",
    "    We are given nonlinear least squares problem of the form\n",
    "    $$\n",
    "    NLS: \\quad \\min_{x\\in \\mathbb{R}^n} h(x) \\doteq \\frac{1}{2} \\big|\\big| r(x) \\big|\\big|^2_2\n",
    "    $$\n",
    "    where \n",
    "    $$\n",
    "    r(x) = \\big(10(x_2 - x^2_1),1 - x_1\\big)^T\n",
    "    $$\n",
    "    It is possible to solve the problem using the Conjugate Gradient Method, Gauss-Newton Algorithm or Plain Gradient Descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## __Project structure__\n",
    "    \n",
    "    .\n",
    "    |\n",
    "    |\n",
    "    ├── src   # Contains algorithm and function implementations\n",
    "    |   ├── conjugategradient.hpp    # Fletcher-Reeves Conjugate Gradient Algorithm\n",
    "    |   ├── functions.cpp    # Implementations of functions and their gradients\n",
    "    |   ├── functions.hpp    # Function headers\n",
    "    |   ├── gaussnewton.hpp    # Gauss-Newton algorithm for Linear least squares problems\n",
    "    |   ├── gradientdescent.hpp    # Plain gradient descent optimizer\n",
    "    |   ├── optimizer.hpp    # Base class implementing backtracking line search and print functionality\n",
    "    |   └── returnvalue.hpp    # Result data container\n",
    "    |\n",
    "    |\n",
    "    └── main    # Main driver for the example problems"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
